{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc1fb7cf-eada-4b23-aa89-448fde9cf1fa",
   "metadata": {},
   "source": [
    "#  ENGR7761 Computer vision project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd4b31-fda3-4b48-a3d2-8ba6c13ef434",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the video and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae1dea38-79f2-45d4-82c9-4e3b06fde2be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of video. Exiting ...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture('SlowMotionDartsThrow.mp4')\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened(): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "# Read the first frame to get its shape\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    flag, img = cap.read()\n",
    "    \n",
    "    if not flag: # Check if there is data in the frame\n",
    "        print(\"End of video. Exiting ...\")\n",
    "        break\n",
    "        \n",
    "    cv2.imshow(\"Default video\", img)\n",
    "    # Press 'q' on keyboard to exit\n",
    "    if cv2.waitKey(10) == ord('q'):\n",
    "        break\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a965e-dbc2-4ffc-b434-71b02b443730",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Apply Gaussian blur to the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1404e7c2-8582-4c66-9d13-1a4500c00197",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of video. Exiting ...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture('SlowMotionDartsThrow.mp4')\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened(): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "# Read the first frame to get its shape\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "    exit()\n",
    "\n",
    "\"\"\" Saving the modified video \"\"\"\n",
    "frame_height, frame_width = frame.shape[:2] # Size to save as (same as input)\n",
    "fps = 30.0 # Frames per second\n",
    "fourcc = cv2.VideoWriter_fourcc(*'H264') # H264 video format\n",
    "out = cv2.VideoWriter('BLurredVideo.mp4', fourcc, fps, (frame_width, frame_height)) # Output file name\n",
    "    \n",
    "    \n",
    "# Blur parameters\n",
    "kernel_size = 21\n",
    "\n",
    "    \n",
    "while True:\n",
    "    flag, img = cap.read()\n",
    "    \n",
    "    if not flag: # Check if there is data in the frame\n",
    "        print(\"End of video. Exiting ...\")\n",
    "        break\n",
    "    blurred = cv2.GaussianBlur(img, (kernel_size, kernel_size), 10)\n",
    "    output_image = blurred.copy()\n",
    "    \n",
    "    # Save the frame into the 'BlurredVideo.mp4' file\n",
    "    out.write(output_image)\n",
    "    \n",
    "    # Display on the video\n",
    "    cv2.imshow(\"Blurred video\", output_image)\n",
    "    \n",
    "    # Press 'q' on keyboard to exit\n",
    "    if cv2.waitKey(10) == ord('q'):\n",
    "        break\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19faec6-48af-459f-8b93-942dbce22e06",
   "metadata": {},
   "source": [
    "## Apply average filter to the blurred video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65c2c79b-b256-4cad-a695-dccef3c03a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of video. Exiting ...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture('BLurredVideo.mp4')\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened(): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "# Read the first frame to get its shape\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "    exit()\n",
    "\n",
    "\"\"\" Saving the modified video \"\"\" \n",
    "frame_height, frame_width = frame.shape[:2] # Size to save as (same as input)\n",
    "fps = 30.0 # Frames per second\n",
    "fourcc = cv2.VideoWriter_fourcc(*'H264') # H264 video format\n",
    "out = cv2.VideoWriter('MovingAvgBLurredVideo.mp4', fourcc, fps, (frame_width, frame_height)) # Output file name\n",
    "    \n",
    "    \n",
    "# Processing parameters\n",
    "fbuffer = 13\n",
    "alpha = float(1.0 / fbuffer)\n",
    "difference_thresh = 4\n",
    "flag, img = cap.read()\n",
    "if flag:\n",
    "    movingaverage = np.float32(img)\n",
    "while True:\n",
    "    flag, img = cap.read()\n",
    "    \n",
    "    if not flag: # Check if there is data in the frame\n",
    "        print(\"End of video. Exiting ...\")\n",
    "        break\n",
    "    \n",
    "    # Calculated the weigthed average of the currrent frame\n",
    "    cv2.accumulateWeighted(img, movingaverage, alpha)\n",
    "    res = cv2.convertScaleAbs(movingaverage)\n",
    "    difference_img = cv2.absdiff(res, img)\n",
    "    \n",
    "    # Perform binary threshold\n",
    "    grey_difference_img = cv2.cvtColor(difference_img, cv2.COLOR_BGR2GRAY) # Convert to greyscale\n",
    "    ret, motionmask = cv2.threshold(grey_difference_img, difference_thresh, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # \n",
    "    output_image = motionmask.copy()\n",
    "    \n",
    "    # Save the frame into the 'MovingAvgBLurredVideo.mp4' file\n",
    "    out.write(output_image)\n",
    "    \n",
    "    # Display on the video\n",
    "    cv2.imshow(\"Moving average blurred video\", output_image)\n",
    "    \n",
    "    # Press 'q' on keyboard to exit\n",
    "    if cv2.waitKey(10) == ord('q'):\n",
    "        break\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b282f5a-616b-487c-8d04-35fd0d3d7797",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Apply Kanade–Lucas–Tomasi feature tracker to the pre-processed video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46bc0ee8-94c3-4e10-950a-c6bf716988fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't receive frame (stream end?). Exiting ...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture('MovingAvgBLurredVideo.mp4') # Load the first video\n",
    "cap2 = cv2.VideoCapture('SlowMotionDartsThrow.mp4') # Load the second video\n",
    "\n",
    "# Check if video works opened successfully\n",
    "if not cap.isOpened(): \n",
    "    print(\"Error opening video stream or file\")\n",
    "if not cap2.isOpened(): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "# Read the first frame to get its shape\n",
    "ret, frame = cap.read() # Read the first frame of the first video\n",
    "if not ret:\n",
    "    print(\"Can't receive frame (stream end?). Exiting ...\") # Print error message if the first frame couldn't be read\n",
    "    exit()\n",
    "# Read the first frame to get its shape\n",
    "ret2, frame2 = cap2.read() # Read the first frame of the second video\n",
    "if not ret2:\n",
    "    print(\"Can't receive frame (stream end?). Exiting ...\") # Print error message if the first frame couldn't be read\n",
    "    exit()\n",
    "\n",
    "# Saving the modified video\n",
    "frame_height, frame_width = frame.shape[:2] # Get the height and width of the frame\n",
    "fps = 30.0 # Set the frames per second\n",
    "fourcc = cv2.VideoWriter_fourcc(*'H264') # Set the video format\n",
    "out = cv2.VideoWriter('KLTTrackedDart.mp4', fourcc, fps, (frame_width, frame_height)) # Create a VideoWriter object for the first output video\n",
    "out2 = cv2.VideoWriter('KLTTrackedDartOverlayOriginal.mp4', fourcc, fps, (frame_width, frame_height)) # Create a VideoWriter object for the second output video\n",
    "\n",
    "# Parameters for ShiTomasi corner detection\n",
    "feature_params = dict(maxCorners = 3, qualityLevel = 0.3, minDistance = 50, blockSize = 5) # Set the parameters for ShiTomasi corner detection\n",
    "\n",
    "# Parameters for Lucas-Kanade optical flow\n",
    "WindowSize = (25, 25) # Set the window size\n",
    "lk_params = dict(winSize = WindowSize, maxLevel = 2, criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 20, 0.03)) # Set the parameters for Lucas-Kanade optical flow\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = None\n",
    "\n",
    "# Initialize old_gray variable\n",
    "old_gray = None\n",
    "    \n",
    "while True:\n",
    "    ret, frame = cap.read() # Read the next frame from the first video\n",
    "    ret2, frame2 = cap2.read() # Read the next frame from the second video\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\") # Print error message if the frame couldn't be read\n",
    "        break\n",
    "    if not ret2:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\") # Print error message if the frame couldn't be read\n",
    "        break\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # Convert the frame to grayscale\n",
    "\n",
    "    # If this is the first frame or no good points were found in the last frame\n",
    "    if mask is None or good_new.size == 0:\n",
    "        p0 = cv2.goodFeaturesToTrack(frame_gray, mask = None, **feature_params) # Detect good features to track\n",
    "        if p0 is None:\n",
    "            continue\n",
    "        mask = np.zeros_like(frame) # Create a mask of zeros with the same size as the frame\n",
    "\n",
    "    # If this is the first frame, initialize old_gray\n",
    "    if old_gray is None:\n",
    "        old_gray = frame_gray.copy() # Copy the current grayscale frame to old_gray\n",
    "\n",
    "    # Calculate optical flow\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params) # Calculate the optical flow\n",
    "\n",
    "    # Select good points\n",
    "    good_new = p1[st==1] # Select the new points where the status is 1\n",
    "    good_old = p0[st==1] # Select the old points where the status is 1\n",
    "\n",
    "    # Draw the tracks\n",
    "    for i,(new,old) in enumerate(zip(good_new,good_old)): # For each pair of new and old points\n",
    "        a,b = map(int, new.ravel()) # Get the coordinates of the new point\n",
    "        c,d = map(int, old.ravel()) # Get the coordinates of the old point\n",
    "        mask = cv2.line(mask, (a,b),(c,d), (0,255,0), 2) # Draw a line from the old point to the new point on the mask\n",
    "        frame = cv2.circle(frame,(a,b),5,(0,0,255),-1) # Draw a circle at the new point on the frame\n",
    "        \n",
    "    # Mask the image to the frame\n",
    "    OriginalMask = cv2.add(frame2,mask) # Add the mask to the orignal frame\n",
    "    out2.write(OriginalMask) # Write the result to the second output video\n",
    "    img = cv2.add(frame,mask) # Add the mask to the motion thing frame\n",
    "    output_image =img.copy() # Copy the result\n",
    "    out.write(output_image) # Write the frame to vid\n",
    "    cv2.imshow('frame',img) # Print \n",
    "    k = cv2.waitKey(15) & 0xff # Wait for a key press\n",
    "    if k == 27: # If the ESC key was pressed\n",
    "        break # Break the loop\n",
    "    \n",
    "    # Update frames\n",
    "    old_gray = frame_gray.copy() # Copy the current grayscale frame to old_gray\n",
    "    p0 = good_new.reshape(-1,1,2) # Reshape good_new and assign it to p0\n",
    "\n",
    "cv2.destroyAllWindows() # Destroy all windows\n",
    "out.release() # Stop recording\n",
    "out2.release() # Stop recording\n",
    "cap.release() # Release the first VideoCapture object\n",
    "cap2.release() # Release the 2nd VideoCapture object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b9d3f-f2ff-48f9-abb6-2db9f3f3d966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
